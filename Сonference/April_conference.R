library('tidyverse')
library('ggplot2')
library('tidytext')
library('widyr')
library('ggraph')
library('igraph')
library('ggplot2')
library('foreign')
library('readxl')
library('intergraph')
library('extrafont')
library('gridExtra')
library('reshape2')
library('wordcloud')    
library('wordcloud2')
library('RColorBrewer')
library('radarchart')
library('stopwords') 
library('dplyr')
library('tibble')

# ---- Databases ----

covid <- read_excel("fake_new_dataset.xlsx")
covid_true <- subset(covid, subset = covid$label == "1")
covid_fake <- subset(covid, subset = covid$label == "0")

afinn <- read_csv("Afinn.csv",
                  col_types = cols(word = col_character(), value = col_double()))
bing <- read_csv("Bing.csv",
                 col_types = cols(word = col_character(), sentiment = col_character()))
nrc <- read_csv("NRC.csv",
                col_types = cols(word = col_character(), sentiment = col_character()))Ð¯

# ---- Most frequent words ----

covid_fake_new <- covid_fake %>%
  filter(!is.na(title)) %>%
  transmute(post_id = row_number(), title) %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words, by = "word") %>%
  filter(word != "de",
         str_detect(word, "[a-z]"))

stopword <- as_tibble(stopwords::stopwords("en")) 
stopword <- rename(stopword, word=value)
covid_fake_new_clean <- anti_join(covid_fake_new, stopword, by = 'word')
newstopwords <- tibble(word = c('corona', 'virus', 'coronavirus', 'ncov', 'covid'))
covid_fake_new_clean <- anti_join(covid_fake_new_clean, newstopwords, by = "word")

covid_fake_new_clean %>%
  count(word, sort = TRUE) %>%
  mutate(word = fct_reorder(word, n)) %>%
  head(20) %>%
  ggplot(aes(word, n))+
  geom_col(width = 0.75,
           col = "white",
           fill = "#99d8c9")+
  coord_flip()+
  labs(title = "Common words in fake news titles")+
  theme_classic()+
  theme(text = element_text(family = "Times", face = "bold", size = 14))

covid_fake_filtered <- covid_fake_new_clean %>%
  add_count(word) %>%
  filter(n >= 10)

covid_fake_all <- covid_fake_filtered %>%
  select(post_id, word) %>%
  pairwise_cor(word, post_id, sort = TRUE) 

top_word_covid_fake <- covid_fake_filtered %>%
  select(post_id, word) %>%
  pairwise_cor(word, post_id, sort = TRUE) %>%
  head(100)



covid_true_new <- covid_true %>%
  filter(!is.na(title)) %>%
  transmute(post_id = row_number(), title) %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words, by = "word") %>%
  filter(word != "de",
         str_detect(word, "[a-z]"))

stopword <- as_tibble(stopwords::stopwords("en")) 
stopword <- rename(stopword, word=value)
covid_true_new_clean <- anti_join(covid_true_new, stopword, by = 'word')
newstopwords <- tibble(word = c('corona', 'virus', 'coronavirus', 'ncov', 'covid'))
covid_true_new_clean <- anti_join(covid_true_new_clean, newstopwords, by = "word")

covid_true_new_clean %>%
  count(word, sort = TRUE) %>%
  mutate(word = fct_reorder(word, n)) %>%
  head(20) %>%
  ggplot(aes(word, n))+
  geom_col(width = 0.75,
           col = "white",
           fill = "#99d8c9")+
  coord_flip()+
  labs(title = "Common words in true news titles")+
  theme_classic()+
  theme(text = element_text(family = "Times", face = "bold", size = 14))

covid_true_filtered <- covid_true_new_clean %>%
  add_count(word) %>%
  filter(n >= 10)

top_word_covid_true <- covid_true_filtered %>%
  select(post_id, word) %>%
  pairwise_cor(word, post_id, sort = TRUE) %>%
  head(100)

# ---- Fake graphs ----

top_word_covid_fake %>%
  graph_from_data_frame() %>%
  ggraph(layout = 'nicely')+
  geom_edge_link()+
  geom_node_point()+
  theme_void(base_size = 18)+
  geom_node_text(aes(label = name), repel = TRUE)

top_word_covid_fake %>%
  graph_from_data_frame() %>%
  ggraph(layout = 'linear', circular = TRUE)+
  geom_edge_arc(aes(colour=correlation, alpha=correlation))+
  coord_fixed()+
  geom_node_text(aes(label = name), repel = TRUE)+
  geom_node_point()+
  theme_void(base_size = 18)+
  scale_edge_color_continuous(low="red", high="green")+
  theme(legend.position = "none")

# ---- Reliable graphs ----

top_word_covid_true %>%
  graph_from_data_frame() %>%
  ggraph(layout = 'nicely')+
  geom_edge_link()+
  geom_node_point()+
  theme_void(base_size = 18)+
  geom_node_text(aes(label = name), repel = TRUE)

top_word_covid_true %>%
  graph_from_data_frame() %>%
  ggraph(layout = 'linear', circular = TRUE)+
  geom_edge_arc(aes(colour=correlation, alpha=correlation))+
  coord_fixed()+
  geom_node_text(aes(label = name), repel = TRUE)+
  geom_node_point()+
  theme_void(base_size = 18)+
  scale_edge_color_continuous(low="red", high="green")+
  theme(legend.position = "none")

# ---- Sentimental analysis for fakes----

options(repr.plot.width=15, repr.plot.height=15)
colsR_B<- brewer.pal(4,name = 'RdBu')
display.brewer.pal(4,name = 'RdBu')

covid_fake_new_clean %>% 
  inner_join(bing, by="word") %>%
  count(word, sentiment, sort=T) %>% 
  acast(word ~ sentiment, value.var = "n", fill=0) %>% 
  comparison.cloud(colors=colsR_B[c(1, 4)], max.words = 400, title.size = 2,
                    scale = c(3,.5))


# ---- Sentimental analysis for reliable ----

covid_true_new_clean %>% 
  inner_join(bing, by="word") %>%
  count(word, sentiment, sort=T) %>% 
  acast(word ~ sentiment, value.var = "n", fill=0) %>% 
  comparison.cloud(colors=colsR_B[c(1, 4)], max.words = 400, title.size = 2,
                   scale = c(3,.5))


# ---- Radar Chart ----

covid_true_new1<-covid_true_new_clean
covid_fake_new1<-covid_fake_new_clean

FAKE <-c()
for (i in 1:length(covid_fake_new1$post_id)){
  FAKE<- append(FAKE,'FAKE')
}
FAKE

RIGHT <-c()
for (i in 1:length(covid_true_new1$post_id)){
  RIGHT<- append(RIGHT,'TRUE')
}
RIGHT

covid_fake_new1$status <- FAKE 
covid_true_new1$status<-RIGHT

covid_new<-rbind(covid_fake_new1, covid_true_new1)

char_sentiment <- covid_new %>% 
  inner_join(nrc, "word") %>% 
  filter(!sentiment %in% c("positive", "negative")) %>% 
  group_by(status, sentiment) %>% 
  count(status, sentiment) %>% 
  select(status, sentiment, char_sentiment_count=n)

total_char <- covid_new %>% 
  inner_join(nrc, "word") %>% 
  filter(!sentiment %in% c("positive", "negative")) %>% 
  count(status) %>% 
  select(status, total=n)

plt <- char_sentiment %>% 
  inner_join(total_char, by="status") %>% 
  mutate(percent = char_sentiment_count / total * 100 ) %>% 
  select(-char_sentiment_count, -total) %>% 
  spread(status, percent)  %>% 
  chartJSRadar(showToolTipLabel = T, maxScale=25, responsive=T,
               addDots = T, 
               colMatrix = grDevices::col2rgb(colsR_B[c(1, 4)]),
               lineAlpha = 0.7, polyAlpha = 0.2)
plt
colsR_B[c(1, 4)]


# ---- ERGM ----

detach(package:igraph)
detach(package:ergm)
detach(package:network)
detach(package:sna)
library('network')
library('sna')
library('ergm')
#library('igraph')

